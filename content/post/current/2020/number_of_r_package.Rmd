---
title: "inSilecoMisc 0.4.0 (part 2/2)"
author: [kevin]
reviewer: [david]
date: 2020-04-21
tags: [R, package, development, inSilecoMisc]
rpkgs: [curl, graphics, utils]
tweet: "inSilecoMisc 0.4.0 (part 2/2)"
draft: true
rbloggers: true
estime: 7
output:
  rmarkdown::html_page:
    toc: true
    fig_width: 3
    dev: svg
---


```R
#' Get the number of R packages
#'
#' This script go and find on the CRAN the package and the first release recorded in the archives.
#' It takes some minutes to get all the information and then it returns a figures that displays the number of packages.
#'
# created: 2015/06/11 modified: 2015/11/20
#'
#' R version 3.2.2 (2015-08-14)
#' Platform: x86_64-apple-darwin15.0.0 (64-bit)
#' Running under: OS X 10.11.1 (El Capitan)
```

I revisited a script I wrote years back (2015/11/20 in and last run R version 3.2.2 (2015-08-14)) with RCURL and even have a todo list :)


My goal is to used web scrapping approaches to assigned a first release archived date number of release using web scrapping only!

```R
library(curl)
con <- curl("http://cran.r-project.org/src/contrib/Archive/")
open(con)
cran_arcv0 <- readLines(con)
close(con)
```
Remove first few lines
<!-- show what it looks like -->

```R
cran_arcv <- cran_arcv0[grepl("^<tr>.*<a.*>.*/</a>.*</tr>$", cran_arcv0)]
pkg_nms <- unlist(
  lapply(cran_arcv, function(x) sub(".*<a.*>(.*)/</a>.*", "\\1", x))
)
```

Maybe worth saying a few words about how `sub()` has been used here.


```R
pkg_nms_a <- available.packages()[, 1]
```

```R
archived <- pkg_nms[!pkg_nms %in% pkg_nms_a]
first <- pkg_nms_a[!pkg_nms_a %in% pkg_nms]
several <- pkg_nms_a[pkg_nms_a %in% pkg_nms]
```

```R
# ntotal package
length(archived) + length(first) + length(several)

#
sum(archived %in% several)
sum(several %in% first)
sum(archived %in% first)

#
df_archived <- data.frame(name = archived,
  first = NA, last = NA, archived_on = NA)
df_first <- data.frame(name = archived,
  first = NA, last = NA, archived_on = NA)
df_several <- data.frame(name = archived,
  first = NA, last = NA, archived_on = NA)
```

```R
## Get the date associated to the first recorded release of every package
getPkDate <- function(x, quiet = FALSE){
  adr <- paste0("http://cran.r-project.org/src/contrib/Archive/", x, "/")
  if (!quiet) print(adr)
  connec <- textConnection(getURL(curl))
  webpg <- readLines(connec)
  close(connec)
  num <- 10 #which(lapply(webpg, function(x) grep("tar.gz",x))==1)
  return(strsplit(webpg[num], "<td align=\"right\">|</td>| ")[[1]][11])
}
##
getPkgInfo <- function(x, quiet = FALSE) {
  adr <- paste0("http://cran.r-project.org/src/contrib/Archive/", x)
  if (!quiet) print(adr)
  con <- curl(adr)
  open(con)
  webpg <- readLines(con)
  close(con)
  # num <- 10 #which(lapply(webpg, function(x) grep("tar.gz",x))==1)
  # return(strsplit(webpg[num], "<td align=\"right\">|</td>| ")[[1]][11])
  webpg
}

# I faced trouble woth the lapply version; Therefor I uses a loop
# pkarv <- lapply(websl2, getPkDate)
pkarv <- list()
for (i in 1:length(websl2)){
  print(i)
  pkarv[[i]] <- getPkDate(websl2[[i]], quiet=TRUE)
}


pkarv_dateparts <- lapply(pkarv, strsplit, split="-")
##
pkarv_day <- as.integer(unlist(lapply(pkarv_dateparts, function(x) x[[1]][1])))
pkarv_month <- charmatch(unlist(lapply(pkarv_dateparts, function(x) x[[1]][2])), month.name)
pkarv_year <- as.integer(unlist(lapply(pkarv_dateparts, function(x) x[[1]][3])))
##
datwipk <- data.frame(pk_name=unlist(websl2), day=pkarv_day, month=pkarv_month, year=pkarv_year, type=as.factor("wipk"))
# head(datwipk)
# Optionnaly you can save it
# save(datwipk, file="./datwipk.rdata")
# Once saved, you can skip the first section
# load("./datwipk.rdata")




#### II- Names that are available but not found in the archives and thus presumly referring to recent packages

## Get the packages names currently available
avlpk <- available.packages(contrib.url("https://mirror.ibcp.fr/pub/CRAN"))
pkname <- as.list(avlpk[,1])
avails <- pkname[which(pkname%in%unlist(websl2)==0)]


getPkDate2 <- function(x, quiet=FALSE){
    adr <- paste0("http://cran.r-project.org/web/packages/",x,"/index.html")
    if (!quiet) print(adr)
    connec <- textConnection(getURL(adr))
    webpg <- readLines(connec)
    close(connec)
    num <- which(lapply(webpg, function(x) grep("Published:",x))==1)
    return(strsplit(webpg[num+1],"td>|<")[[1]][3])
}

pknoarv <- lapply(avails, getPkDate2)
##
pknoarv_dateparts <- lapply(pknoarv, strsplit, split="-")
##
pknoarv_day <- as.integer(unlist(lapply(pknoarv_dateparts, function(x) x[[1]][3])))
pknoarv_month <- as.integer(unlist(lapply(pknoarv_dateparts, function(x) x[[1]][2])))
pknoarv_year <- as.integer(unlist(lapply(pknoarv_dateparts, function(x) x[[1]][1])))
##
datnopk <- data.frame(pk_name=unlist(avails), day=pknoarv_day, month=pknoarv_month, year=pknoarv_year, type=as.factor("nopk"))

# Optionnaly you can save it
# save(datnopk, file="./datnopka.rdata")
##
#load("./datnopk.rdata")


#### III- datawipk + datanopk
#load("~/Documents/Codes/R_workspace/datnopk.rdata")
datpk <- rbind(datnopk, datwipk)
datpk <- datpk[order(datpk[,1]),]
# count <- rep(1,nrow=length(datpk))
save(datpk, file="./datpk.rdata")
load("./datpk.rdata")



#### IV- Graph

fyear <- sort(unique(datpk$year))
nbpk <- table(datpk$year)
cumpk <- cumsum(nbpk)

col1 <- "grey30"
col2 <- "#CC001D"

par(cex.lab=1.4, mar=c(5,5,1,5))
plot(fyear, nbpk, type="h", lwd=8, axes=FALSE, col=col1, ylab="", xlab="Year")
axis(1)
axis(2)
mtext(side=2, text="Number of packages", cex=par()$cex.lab, line=2.8, col=col1)
box(bty="u", lwd=1.4)
par(new=TRUE)
plot(fyear, cumpk, axes=FALSE, type="l", lwd=2, col=col2, ann=FALSE)
axis(4)
mtext(side=4, text="Cumulative number of packages", cex=par()$cex.lab, line=2.8, col=col2)
mtext(side=1, at=2015, text=paste0("generated on ",format(Sys.time(), "%b %d %Y")), cex=0.8, line=3.5)




## TODO
## - improve the code
## - search for Maintainers and then find packages that are no longer maintained
## - few code lines I have started
# mntr <- list()
# ### Maintainer
# for (i in 1:100){
#     adr <- paste0("http://cran.r-project.org/web/packages/", pknoarv[i],"/index.html")
#     print(c(i,adr))
#     connec <- textConnection(getURL(adr))
#     webpg <- readLines(connec)
#     close(connec)
#     num <- which(lapply(webpg, function(x) grep("Maintainer:",x))==1)
#     mntr[[i]] <- strsplit(webpg[num+1],"<td>| &")[[1]][2]
#     # num <- which(lapply(webpg, function(x) grep("Author:",x))==1)
#     # mntr[[i]] <- strsplit(webpg[num+1],"<td>|</td>")[[1]]
# }



Problem with this estimatios:

if archived for sometime and the re-actualise some year without it correct!

There are better way to proceed run available packages every day



https://stat.ethz.ch/R-manual/R-devel/library/tools/html/00Index.html

http://cran-logs.rstudio.com/

https://github.com/r-hub/cranlogs

https://github.com/rsquaredacademy/pkginfo


https://blog.revolutionanalytics.com/2017/01/cran-10000.html

https://stat.ethz.ch/R-manual/R-devel/library/tools/html/00Index.html


https://cran.r-project.org/web/packages/rasciidoc/index.html

https://blog.revolutionanalytics.com/2009/03/cranberries-keep-uptodate-with-r-packages.html

https://twitter.com/CRANberriesFeed


http://dirk.eddelbuettel.com/cranberries/about/


I did created A minimal version of the great cranberries uisng GH actions
http://dirk.eddelbuettel.com/cranberries/about/

